{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project Part A: Model Performance and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, you shall develop two separate natural language processing machine learning pipelines. The first shall target sentiment analysis, while the second shall target question answering. Throughout this process, you will compare different models on tasks, and learn to optimize hyperparameters effectively.\n",
    "\n",
    "Note: We believe that this project may be more challenging than previous projects due to the number of libraries and tools involved. It is even more important to start early and iterate early here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Sentiment Analysis with Python\n",
    "In this part, you will apply sklearn and related NLP libraries to predict user movie review sentiment on the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/). Before you begin, check that your installed `scikit-learn` version is as specified in `requirements.txt`; otherwise you may not pass the local tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading a subset of the dataset, which contains 5000 movie reviews and their associated sentiment labels (i.e., whether a review is considered positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"imdb_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taran Adarsh a reputed critic praised such a d...</td>\n",
       "      <td>taran adarsh repute critic praise dubba movie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>worth entertainment value rental especially li...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I liked Antz, but loved \"A Bug's Life\". The an...</td>\n",
       "      <td>like antz love bug life animation put paid def...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This reboot is like a processed McDonald's mea...</td>\n",
       "      <td>reboot like process mcdonald meal compare ang ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The working title was: \"Don't Spank Baby\". &lt;br...</td>\n",
       "      <td>work title spank baby wayne crawford go become...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Taran Adarsh a reputed critic praised such a d...   \n",
       "1  Worth the entertainment value of a rental, esp...   \n",
       "2  I liked Antz, but loved \"A Bug's Life\". The an...   \n",
       "3  This reboot is like a processed McDonald's mea...   \n",
       "4  The working title was: \"Don't Spank Baby\". <br...   \n",
       "\n",
       "                                    processed_review sentiment  \n",
       "0  taran adarsh repute critic praise dubba movie ...  negative  \n",
       "1  worth entertainment value rental especially li...  negative  \n",
       "2  like antz love bug life animation put paid def...  positive  \n",
       "3  reboot like process mcdonald meal compare ang ...  negative  \n",
       "4  work title spank baby wayne crawford go become...  positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will be ignored by the autograder\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `review` column contains raw review texts from the original dataset. However, it's always a good idea to process and clean text data before performing analysis. To reduce your workload, we have processed the text for you already. The column `processed_review` was constructed by processing and tokenizing the raw reviews, using the `preprocess_text` function from Project 3, and then joining the review tokens by a single space. From this point, you only need to focus on the `processed_review` and `sentiment` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the distribution of class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2500\n",
       "positive    2500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will be ignored by the autograder\n",
    "display(df_reviews['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 2500 positive reviews and 2500 negative reviews. In other words, our dataset is perfectly balanced, and thus we can reasonably use accuracy as our metric of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Count Vectorizer\n",
    "\n",
    "Similar to P3, before we put any of our data into a machine-learning model, we need to convert our text into vectors of information to use through feature engineering. \n",
    "\n",
    "The first feature engineering task we will perform is building a term-frequency matrix. However, as you have already performed this task once manually in P3, we shall use `sklearn`, a standard machine learning library, to make things easier. Implement the function `count_vectorizer` that uses sklearn's `CountVectorizer` API to construct the term-frequency training matrix and testing matrix, along with the feature names (i.e., the list of words corresponding to the columns in the matrices). \n",
    "\n",
    "One point to keep in mind is that `CountVectorizer` will, by default, do its own preprocessing and tokenization (see the [documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) for more details). As these steps have already performed, we will need to overwrite sklearn's default behaviors by specifying that `analyzer` should be `str.split`.\n",
    " \n",
    "Additionally, be careful with how you transform the test data. Unlike P3, where we ignored this step for simplicity, we want to fit our transformation tool **only on the training data**, but transform both the training and test data. This means that, if a token is in the test data but not in the training data, it will be ignored.\n",
    "\n",
    "Notes:\n",
    "* If you are using an earlier version of scikit learn while testing, you will need to use **get_feature_names()** instead of **get_feature_names_out()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the term-frequency matrices for train_data and test_data using CountVectorizer.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "        \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : TF matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : TF matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the TF matrices\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(analyzer=(lambda x: x.split()))\n",
    "    X = vectorizer.fit_transform(reviews_train).toarray()\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    tf_train = sp.csr_matrix(X)\n",
    "    \n",
    "    if reviews_test is not None:\n",
    "        X_test = vectorizer.transform(reviews_test)\n",
    "        tf_test = sp.csr_matrix(X_test)\n",
    "    else:\n",
    "        tf_test = None\n",
    "\n",
    "    return tf_train, tf_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_count_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    count_vec_train, count_vec_test, features = count_vectorizer(reviews_train, reviews_test)\n",
    "    assert count_vec_train.shape == (3750, 27242)\n",
    "    assert count_vec_test.shape == (1250, 27242)\n",
    "    assert np.allclose(\n",
    "        count_vec_train.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [70, 65, 168, 77, 139, 132, 28, 139, 453, 89]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        count_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [168, 60, 59, 144, 494, 135, 69, 119, 76, 68]\n",
    "    )\n",
    "    assert list(features[:10]) == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert list(features[-10:]) == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_count_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Bag of Words Vectorizer\n",
    "\n",
    "Using the last function, let's also implement a so-called \"Bag of Words\" vectorizer. This vectorizer only considers the presence or absence of a token instead of the count. This makes it considerably more interpretable than the Count Vectorizer, at the cost of not including the frequency of potentially useful terms instead.  Implement the function `bow_vectorizer` that constructs a bag-of-words training and testing matrices, along with the feature names (i.e., the list of words corresponding to the columns in the matrices).  You might want to use the solution to the previous question to handle this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the bag of words matrices for train_data and test_data.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "    \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : Bag-of-Words matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : Bag-of-Words matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the Bag-of-Words matrices\n",
    "    \"\"\"\n",
    "    count_train, count_test, features = count_vectorizer(reviews_train, reviews_test)\n",
    "    count_train[count_train.nonzero()] = 1 #count_train[count_train.nonzero()]/count_train[count_train.nonzero()]\n",
    "    if reviews_test is not None:\n",
    "        count_test[count_test.nonzero()] = 1 # count_test[count_test.nonzero()]/count_test[count_test.nonzero()]\n",
    "    else:\n",
    "        count_test = None\n",
    "    return count_train, count_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_bow_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    bow_vec_trains, bow_vec_test, features = bow_vectorizer(reviews_train, reviews_test)\n",
    "    assert bow_vec_trains.shape == (3750, 27242)\n",
    "    assert bow_vec_test.shape == (1250, 27242)\n",
    "    \n",
    "    assert np.allclose(\n",
    "        bow_vec_trains.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [59, 64, 155, 62, 110, 110, 23, 119, 310, 64]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        bow_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [113, 38, 52, 123, 258, 109, 59, 101, 74, 56]\n",
    "    )\n",
    "    assert list(features[:10]) == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert list(features[-10:]) == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_bow_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: TF-IDF Vectorizer\n",
    "\n",
    "Now let's also do the same with tf-idf, like we did in the last project. Implement the function `tfidf_vectorizer` that uses sklearn's `TfidfVectorizer` API to construct the TF-IDF training matrix and testing matrices, along with the feature names (i.e., the list of words corresponding to the columns in the matrices). Use the same parameter value for `analyzer` as you did in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF matrices for train_data and test_data using TfidfVectorizer.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "    \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : TF-IDF matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : TF-IDF matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the TF-IDF matrices\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(analyzer=(lambda x: x.split()))\n",
    "    X = vectorizer.fit_transform(reviews_train).toarray()\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    tf_train = sp.csr_matrix(X)\n",
    "    \n",
    "    if reviews_test is not None:\n",
    "        X_test = vectorizer.transform(reviews_test).toarray()\n",
    "        tf_test = sp.csr_matrix(X_test)\n",
    "    \n",
    "    return tf_train, tf_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_tfidf_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    tfidf_vec_trains, tfidf_vec_test, features = tfidf_vectorizer(reviews_train, reviews_test)\n",
    "    assert tfidf_vec_trains.shape == (3750, 27242)\n",
    "    assert tfidf_vec_test.shape == (1250, 27242)\n",
    "    assert np.allclose(\n",
    "        tfidf_vec_trains.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [7.03658925089979, 7.417196035144321, 11.492434722367015, 6.965673648338525, 9.428219597939362, 9.425632229448961, 3.9722806270035345, 9.635230284023372, 11.779155501275017, 7.44670396016231]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        tfidf_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [7.2233277330801196, 4.869804242110142, 6.249091468966529, 9.689812079503804, 11.89432945296538, 9.115185225757216, 6.798492438570971, 8.57464867777901, 7.954528809138947, 6.81383392701789]\n",
    "    )\n",
    "    assert list(features[:10]) == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert list(features[-10:]) == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_tfidf_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Predicting review sentiment\n",
    "Let's now see which feature construction method -- TF, TF-IDF, or Bag-of-Words -- is better for predicting review sentiments in our dataset. Our choice of learning algorithm here will be a support vector machine with Gaussian kernel (this means that it uses a different hypothesis function that can also account for non-linearly separable data). You can apply this learning algorithm by creating an instance of sklearn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class, with `kernel = \"rbf\"` and `C = 1.0`.\n",
    "\n",
    "Implement the function `predict_sentiment` that takes as input the `reviews` and `sentiment` columns of our IMDB dataset and performs the following tasks:\n",
    "1. Convert the `sentiment` column to a vector `y` of 1s and -1s: `positive` corresponds to 1 and `negative` to -1.\n",
    "1. Perform a [stratified k-fold split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) of the review and sentiment vectors, based on the provided `k`. Also set `shuffle` to `True` and `random_state` to the provided `seed`.\n",
    "1. For $f$ from $1 \\to k$:\n",
    "     * Let fold $f$ be the test set, and the remaining $k-1$ folds be the training set.\n",
    "     * Convert the training and testing reviews to feature matrices `X_train` and `X_test`, using either TF, TF-IDF, or Bag-of-Words. Which method to use is based on the function parameter `method`.\n",
    "     * Train the SVM model on `X_train, y_train` and evaluate its accuracy $a_f$ on `X_test, y_test`.\n",
    "1. Return $a_1, a_2, \\ldots, a_k$.\n",
    "\n",
    "**Notes**:\n",
    "* As a reminder, accuracy is defined as\n",
    "$$\\text{Acc} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}(y^{(i)} = \\hat y^{(i)}).$$\n",
    "You can also use the `score` function from `SVC` to quickly compute accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review_sentiment(reviews, sentiments, method, k, seed = 0):\n",
    "    \"\"\"\n",
    "    Compute the cross-validated accuracy of SVM with either TF or TF-IDF features\n",
    "    in predicting review sentiment.\n",
    "    \n",
    "    args:\n",
    "        reviews (pd.Series[str]) : a Series of all processed movie reviews\n",
    "        sentiments (pd.Series[str]) : a Series of movie review sentiments,\n",
    "            containing either \"positive\" or \"negative\"\n",
    "        method (str) : a string which is either \"TF\", \"TF-IDF\", or \"Bag\"\n",
    "            specifying which feature construction method to use\n",
    "        k (int) : the number of folds in stratified k-fold split\n",
    "    \n",
    "    kwargs:\n",
    "        seed (int) : the random generator seed for kfold split\n",
    "        \n",
    "    return:\n",
    "        List[float] : a list of k accuracy values from evaluating a trained SVM model\n",
    "            on each of the k folds, using the remaining folds as training data\n",
    "    \"\"\"\n",
    "    sentiment_vec = sentiments.apply(lambda x: -1 if x == \"negative\" else 1 if x == \"positive\" else None).to_list()\n",
    "    ## ^^ could have used np.where(x==\"negative\", -1, 1)\n",
    "    skf = StratifiedKFold(n_splits = k, shuffle = True, random_state = seed)\n",
    "    accuracies = []\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(reviews, sentiment_vec)):\n",
    "        reviews_train = reviews[train_index]\n",
    "        reviews_test = reviews[test_index]\n",
    "        if method == 'TF':\n",
    "            X_train, X_test, features = count_vectorizer(reviews_train, reviews_test)\n",
    "        elif method == 'TF-IDF':\n",
    "            X_train, X_test, features = tfidf_vectorizer(reviews_train, reviews_test)\n",
    "        elif method == 'Bag':\n",
    "            X_train, X_test, features = bow_vectorizer(reviews_train, reviews_test)\n",
    "        else:\n",
    "            raise ValueError(\"must be one of 'TF', 'TF-IDF', or 'Bag'\")\n",
    "\n",
    "        y_train = [sentiment_vec[idx] for idx in train_index]\n",
    "        y_test = [sentiment_vec[idx] for idx in test_index]\n",
    "        model = SVC(kernel='rbf', C=1.0).fit(X_train, y_train)\n",
    "        acc = model.score(X_test, y_test)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "Cross-validated accuracy of SVM with TF matrices 0.8402\n",
      "Cross-validated accuracy of SVM with TF-IDF matrices 0.8604\n",
      "Cross-validated accuracy of SVM with Bag-of-Words matrices 0.849\n"
     ]
    }
   ],
   "source": [
    "def test_predict_review_sentiment():\n",
    "    # prediction based on TF\n",
    "    count_vec_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"TF\", 10)\n",
    "    assert count_vec_accs == [0.878, 0.852, 0.85, 0.82, 0.824, 0.824, 0.82, 0.854, 0.848, 0.832], count_vec_accs\n",
    "    \n",
    "    # prediction based on TF-IDF\n",
    "    tf_idf_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"TF-IDF\", 10)\n",
    "    assert tf_idf_accs == [0.88, 0.862, 0.854, 0.866, 0.848, 0.85, 0.848, 0.88, 0.868, 0.848], tf_idf_accs\n",
    "\n",
    "    # prediction based on Bag-of-Words\n",
    "    bow_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"Bag\", 10)\n",
    "    assert bow_accs == [0.888, 0.85, 0.852, 0.832, 0.83, 0.836, 0.842, 0.862, 0.848, 0.85], bow_accs\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "    print(\"Cross-validated accuracy of SVM with TF matrices\", np.mean(count_vec_accs))\n",
    "    print(\"Cross-validated accuracy of SVM with TF-IDF matrices\", np.mean(tf_idf_accs))\n",
    "    print(\"Cross-validated accuracy of SVM with Bag-of-Words matrices\", np.mean(bow_accs))\n",
    "    \n",
    "test_predict_review_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above tests can take a while to run. The reference solution takes around 10 minutes on an Azure `Standard DS2 V3` compute.\n",
    "\n",
    "\n",
    "We see that using TF-IDF features yields better cross-validated accuracy than using TF features or Bag-of-Words features (when the learning algorithm is SVM with RBF kernel and $C = 1.0$), although the difference in this case is not large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Hyper-Parameter Optimization\n",
    "\n",
    "With most machine learning algorithms, you will have the following:\n",
    "- A variety of algorithms you want to assess on a particular dataset\n",
    "- A variety of **hyper-parameters** that you want to evaluate\n",
    "\n",
    "Unlike the **parameters** that are used to determine the model's outputs given its inputs ( what we change through training ), the **hyper-parameters** are inputs to the algorithm that need to be tuned separately. In practice, this is done through **hyper-parameter optimization** techniques, which try different combinations of hyper-parameters and then use those to optimize our performance appropriately. \n",
    "\n",
    "Doing this in a principled way is challenging. As we want some assurance of the performance of any model's hyper-parameters, we'd like to use cross-validation for the hyper-parameter selection as well as selecting a model. To do this in a principled way, one common approach is GridSearch with Nested Cross-Validation, where we perform cross-validation on the search procedure itself. \n",
    "\n",
    "Implement the function `optimize_random_forest` that takes as input the `reviews` and `sentiment` columns of our IMDB dataset and performs the following tasks:\n",
    "\n",
    "1. Convert the `sentiment` column to a vector `y` of 1s and -1s: `positive` corresponds to 1 and `negative` to -1.\n",
    "1. Perform a [stratified k-fold split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) of the review and sentiment vectors, based on the provided `k`. Also set `shuffle` to `True` and `random_state` to the provided `seed`.\n",
    "1. For $f$ from $1 \\to k$:\n",
    "     * Let fold $f$ be the outer test set, and the remaining $k-1$ folds be the outer training set.\n",
    "     * Perform a secondary stratified n-fold \"inner\" split of the outer training set, based on the provided `n`. Again, set `shuffle` to `True` and `random_state` to the provided `seed`.\n",
    "     * For each hyper-parameter configuration, perform nested n-cross-fold validation using a RandomForestClassifier, setting `random_state` to `seed` to ensure deterministic training. Convert the training and testing reviews to feature matrices `X_train` and `X_test`, using either TF or TF-IDF. \n",
    "     * Pick the parameter setting which has the highest accuracy according to the n-cross-fold validation. Train a classifier on the **outer** training set, and record the **outer** test set accuracy.\n",
    "1. Return $a_1, a_2, \\ldots, a_k$, along with the most accurate hyper-parameter combinations found, $c_1, c_2, \\ldots, c_k$\n",
    "\n",
    "Note: For this question **do not use GridSearchCV**. Due to how random state is preserved, and how we are defining our state space, it is not compatible and may result in random state issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest(reviews, sentiments, params, method, k, n, seed = 0):\n",
    "    \"\"\"\n",
    "    Compute the nested cross-fold accuracies and configurations found for a random forest model on the reviews and sentiments.\n",
    "    \n",
    "    args:\n",
    "        reviews (pd.Series[str]) : a Series of all processed movie reviews\n",
    "        sentiments (pd.Series[str]) : a Series of movie review sentiments,\n",
    "            containing either \"positive\" or \"negative\"\n",
    "        params (list[dict[str, Any]]) : a list of parameter configurations to try. The keys of each dict are the exact same as the keyword arguments of RandomForestClassifier.\n",
    "        method (str) : a string which is either \"TF\",\"TF-IDF\", or \"Bag\"\n",
    "            specifying which feature construction method to use\n",
    "        k (int) : the number of folds in the outer stratified k-fold split\n",
    "        n (int) : the number of folds in the inner stratified k-fold split\n",
    "    \n",
    "    kwargs:\n",
    "        seed (int) : the random generator seed for kfold split\n",
    "        \n",
    "    return:\n",
    "        List[Tuple(float, dict[str, Any])] : a list of k accuracy values and configurations from evaluating a trained random forest model for each outer fold.\n",
    "    \"\"\"\n",
    "    # DO NOT REMOVE THE FOLLOWING LINE FOR GRADING\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(1)\n",
    "\n",
    "\n",
    "    # Create the labels array and StratifiedKFold Instance\n",
    "    sentiment_vec = sentiments.apply(lambda x: -1 if x == \"negative\" else 1 if x == \"positive\" else None).to_list()\n",
    "    skf = StratifiedKFold(n_splits = k, shuffle = True, random_state = seed)\n",
    "\n",
    "    # Loop through outer folds, and index accordingly.\n",
    "    outer_accuracy_combinations = []\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(reviews, sentiment_vec)):\n",
    "        outer_reviews_train = reviews[train_index].copy().reset_index(drop=True)\n",
    "        outer_reviews_test = reviews[test_index].copy().reset_index(drop=True)\n",
    "\n",
    "        outer_y_train = [sentiment_vec[idx] for idx in train_index]\n",
    "        outer_y_test = [sentiment_vec[idx] for idx in test_index]\n",
    "\n",
    "        if method == 'TF':\n",
    "            X_train_outer, X_test_outer, features = count_vectorizer(outer_reviews_train, outer_reviews_test)\n",
    "        elif method == 'TF-IDF':\n",
    "            X_train_outer, X_test_outer, features = tfidf_vectorizer(outer_reviews_train, outer_reviews_test)\n",
    "        elif method == 'Bag':\n",
    "            X_train_outer, X_test_outer, features = bow_vectorizer(outer_reviews_train, outer_reviews_test)\n",
    "        else:\n",
    "            raise ValueError(\"must be one of 'TF', 'TF-IDF', or 'Bag'\")\n",
    "    \n",
    "        # Create second stratified K fold instance.\n",
    "        skf_inner = StratifiedKFold(n_splits = n, shuffle = True, random_state = seed)\n",
    "\n",
    "        # For each inner split, train the model\n",
    "        #params_list = []\n",
    "        #inner_accuracies = []\n",
    "        best_param = None\n",
    "        best_accuracy = None\n",
    "        for param in params:\n",
    "            accuracies = []\n",
    "            for j, (inner_train_index, inner_test_index) in enumerate(skf_inner.split(outer_reviews_train, outer_y_train)):\n",
    "                inner_reviews_train = outer_reviews_train[inner_train_index] # reviews[inner_train_index]\n",
    "                inner_reviews_test = outer_reviews_train[inner_test_index] # reviews[inner_test_index]\n",
    "    \n",
    "                inner_y_train = [outer_y_train[idx] for idx in inner_train_index]\n",
    "                inner_y_test = [outer_y_train[idx] for idx in inner_test_index]\n",
    "                \n",
    "                if method == 'TF':\n",
    "                    X_train_inner, X_test_inner, features = count_vectorizer(inner_reviews_train, inner_reviews_test)\n",
    "                elif method == 'TF-IDF':\n",
    "                    X_train_inner, X_test_inner, features = tfidf_vectorizer(inner_reviews_train, inner_reviews_test)\n",
    "                elif method == 'Bag':\n",
    "                    X_train_inner, X_test_inner, features = bow_vectorizer(inner_reviews_train, inner_reviews_test)\n",
    "                else:\n",
    "                    raise ValueError(\"must be one of 'TF', 'TF-IDF', or 'Bag'\")\n",
    "    \n",
    "                inner_model = RandomForestClassifier(random_state=seed, **param)\n",
    "                inner_model.fit(X_train_inner, inner_y_train)\n",
    "                inner_acc = inner_model.score(X_test_inner, inner_y_test)\n",
    "                accuracies.append(inner_acc)\n",
    "            # Average performance and select best model.\n",
    "            avg_performance = np.mean(accuracies)\n",
    "            if best_accuracy is None:\n",
    "                best_accuracy = avg_performance\n",
    "                best_param = param\n",
    "            elif avg_performance > best_accuracy:\n",
    "                best_accuracy = avg_performance\n",
    "                best_param = param\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #inner_results_df = pd.DataFrame({'param': params_list, 'accuracy': inner_accuracies})\n",
    "        #inner_results_df['param_string'] = inner_results_df['param'].astype(str)\n",
    "        #avg_acc_df = inner_results_df.groupby('param_string').agg({'accuracy': np.mean}).sort_values(by='accuracy', ascending=False).reset_index()\n",
    "        #max_param_string = avg_acc_df['param_string'][0]\n",
    "        #best_param = inner_results_df[inner_results_df['param_string'] == max_param_string]['param'].to_list()[0]\n",
    "        outer_model = RandomForestClassifier(random_state=seed, **best_param).fit(X_train_outer, outer_y_train)\n",
    "        outer_acc = outer_model.score(X_test_outer, outer_y_test)\n",
    "        outer_accuracy_combinations.append((outer_acc, best_param))\n",
    "\n",
    "    return outer_accuracy_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "### NOT SURE WHY THIS DOESN'T PASS........\n",
    "def test_optimize_random_forest():\n",
    "    output = optimize_random_forest(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], [{'n_estimators':100, 'max_depth':None},{'n_estimators':500, 'max_depth':None}, {'n_estimators':1000, 'max_depth':10},{'n_estimators':1000, 'max_depth':100}, {'n_estimators':1000, 'max_depth':None}], \"TF-IDF\", 10, 2)\n",
    "    assert output == [(0.87, {'n_estimators': 1000, 'max_depth': 100}), (0.842, {'n_estimators': 1000, 'max_depth': 100}), (0.86, {'n_estimators': 1000, 'max_depth': None}), (0.824, {'n_estimators': 1000, 'max_depth': 100}), (0.826, {'n_estimators': 1000, 'max_depth': 100}), (0.848, {'n_estimators': 1000, 'max_depth': 10}), (0.856, {'n_estimators': 1000, 'max_depth': None}), (0.858, {'n_estimators': 1000, 'max_depth': 100}), (0.838, {'n_estimators': 1000, 'max_depth': 100}), (0.834, {'n_estimators': 1000, 'max_depth': 100})], output\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_optimize_random_forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Again, the local tests do take significant time, so please plan ahead. On an Azure `Standard DS2 V3` compute, these tests take around 20 minutes to run. If your solution is taking significantly more time, check out the `njobs` argument of RandomForestClassifier. \n",
    "\n",
    "\n",
    "While using this, we can see that our nested cross validation has revealed a few things of note:\n",
    "\n",
    "1. We have too few inner splits. When working with this type of setup, we need more data in the inner split to decide whether setting `max_depth` to `100` or `None` is reasonable for this data, as currently we are incapable of deciding between them.\n",
    "1. It seems reasonable to look at higher `n_estimators` values. While we looked at lower values like `100` and `500`, they did not get chosen, and so it might be reasonable to consider larger values just to see if we get diminishing returns. We know from the conceptual materials that this seems reasonable, as the more estimators are available usually the better a RandomForest does.\n",
    "1. In general, nested cross validation with a fixed set of choices seems rather time intensive. After running this grid, if we wanted to extend this to new sets of choices, we would need to specify a larger grid that would take exponentially more time. There are methods that can speed this up, but in general hyper-parameter optimization is a fairly difficult task.\n",
    "1. Lastly, sklearn's RandomForestClassifier seems ill-suited to this task compared to the SVM we looked at previously. If you look online, Random Forests are usually considered state of the art for small dataset classification tasks yet, despite our extensive set of parameters, we were unable to find a setting that did as well as the SVM. It's important to realize that, no matter what the literature says is hot, all models can only be truly evaluated by testing. While it's possible there is a hyper-parameter setting where Random Forests outperforms SVMs for this featurization, this result is a good reminder that it's very important to be exhaustive in hyper-parameter optimization.\n",
    "\n",
    "Through all of this, we can reasonably say that, if accuracy is our goal, then we should pick the SVC model. However, when deploying any machine learning model, it's possible that accuracy is not our main goal. Let's take the best RandomForestClassifier and compare it with our SVM model in more detail. Let's first train both models on the same training-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf = SVC(kernel = \"rbf\", C = 1.0)\n",
    "rf_clf = RandomForestClassifier(n_estimators = 1000, max_depth = None, random_state = 0, n_jobs = -1)\n",
    "# train test split\n",
    "reviews_train, reviews_test, sentiments_train, sentiments_test = train_test_split(\n",
    "    df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], random_state = 42\n",
    ")\n",
    "# TFIDF vectorizer\n",
    "tfidf_vec_train, tfidf_vec_test, features = tfidf_vectorizer(reviews_train, reviews_test)\n",
    "# fit SVM\n",
    "svc_clf.fit(tfidf_vec_train, np.where(sentiments_train == \"positive\", 1, -1))\n",
    "# fit random forest\n",
    "rf_clf.fit(tfidf_vec_train, np.where(sentiments_train == \"positive\", 1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's use sklearn's `classification_report` to report the precision, recall, f1-score for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.85      0.86       617\n",
      "           1       0.86      0.88      0.87       633\n",
      "\n",
      "    accuracy                           0.87      1250\n",
      "   macro avg       0.87      0.87      0.87      1250\n",
      "weighted avg       0.87      0.87      0.87      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(\n",
    "    np.where(sentiments_test == \"positive\", 1, -1),\n",
    "    svc_clf.predict(tfidf_vec_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.86      0.85       617\n",
      "           1       0.86      0.84      0.85       633\n",
      "\n",
      "    accuracy                           0.85      1250\n",
      "   macro avg       0.85      0.85      0.85      1250\n",
      "weighted avg       0.85      0.85      0.85      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(\n",
    "    np.where(sentiments_test == \"positive\", 1, -1),\n",
    "    rf_clf.predict(tfidf_vec_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Model Selection Under Different Conditions\n",
    "`classification_report` provides us with several metrics and, surprisingly, our SVC model is not completely dominating in all metrics compared to the RandomForest model. Implement the \"use_model_under\" model which returns either \"random forest\" or \"svm\" as the better choice given these model statistics. Refer to the conceptual content for more details about the metrics if you are unsure what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model_under():\n",
    "    \"\"\"\n",
    "    For each situation, pick either \"random forest\" or \"svm\" as better model.\n",
    "    \"\"\"\n",
    "    # Situation 1: You want to make sure that you don't miss any positive reviews.\n",
    "    dont_miss_positive_reviews = \"svm\"\n",
    "    # Situation 2: Assuming class distribution is unchanged, you want to reasonably decide the sentiment of a review.\n",
    "    dont_misclassify_negative_reviews = \"svm\"\n",
    "    # Situation 3: You want to make sure that every review you pick as negative is negative.\n",
    "    dont_misclassify_positive_reviews = \"svm\"\n",
    "    # Situation 4: You want to make sure that you don't miss any negative reviews.\n",
    "    dont_miss_negative_reviews = \"random forest\"\n",
    "    return dont_miss_positive_reviews, dont_misclassify_negative_reviews, dont_misclassify_positive_reviews, dont_miss_negative_reviews\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this local test - it is important to note that this test simply is for testing the format of your answer so you can pass the autograder. Getting the local test correct will **not** lead to correctness in the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_use_model_under():\n",
    "    answers = use_model_under()\n",
    "    assert [\"random forest\" == answer or \"svm\" == answer for answer in answers]\n",
    "\n",
    "test_use_model_under()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that our model development process would have gone differently if we were focusing on a different metric. This comes to the heart of model development, as picking a metric is as important as developing a model. When deploying a model in production, we need to very closely pay attention to what metric we are using, as it will heavily change the model we end up picking.\n",
    "\n",
    "Now let's move onto Part B, where we will change domains to work on another natural language processing pipeline, this time for Question Answering."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
