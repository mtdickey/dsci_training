{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9466dc",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Building upon our Machine-Learning series with the Blog, we will now introduce ways to implement **logistic regression** in Python.  If you are not familiar with this concept already (or even if you need a refresher), we *highly* recommend reading Derivative's more technical explanation of logistic regression and perusing our [recommended materials](#Additional-Learning-Resources) to learn more.\n",
    "\n",
    "The goal of this post is to demonstrate two commonly used modules for building regression models, **scikit-learn** and **statsmodels**.  These modules each provide *a lot* of support for various statistical/machine learning models... so, you may be asking: *\"how are they different?\"*  Good question, reader!  The answer lies in the difference between *Statistics* üßÆÔ∏è and *Machine Learning* ü§ñ.\n",
    "  - **statsmodels** builds models from the perspective of **Statistics**.  This produces more *explanatory* models that can answer questions like \"What is the relationship between X and Y?\".\n",
    "  - **scikit-learn** builds models from the perspective of **Machine Learning**.  The goal of these models is to be as *predictive* as possible and answer the question \"Given X, what should we predict for Y?\"\n",
    "\n",
    "As we will see, both modules are very easy to use, but there are some key differences that you should be aware of.  I hope after reading this post, you are able to choose which module best suits your problem.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "This post is split into the following sections:\n",
    "\n",
    "   1. [Data Setup](#1.-Data-Setup)\n",
    "   2. [Using `statsmodels` to build a logistic regression üßÆÔ∏è](#Using-statsmodels-to-build-a-logistic-regression)\n",
    "   3. [Using `scikit-learn` to build a logistic-regression ü§ñ](#Using-scikit-learn-to-build-a-logistic-regression)\n",
    "   4. [Conclusion](#Conclusion)\n",
    "   5. [Additional Learning Resources](#Additional-Learning-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca70049",
   "metadata": {},
   "source": [
    "### 1. Data Setup\n",
    "\n",
    "Stealing the awesome work of the \"Data Preprocessing\" post, we will be leveraging the *Titanic* üõ•Ô∏è dataset in this lesson.  Our **target variable** will be `Survived` (the indicator of whether a passenger survived), and we will test out the various other fields as features/explanatory variables in the model.\n",
    "\n",
    "Here's what that data looked like, for a reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv(\"resources/titanic-output.csv\")\n",
    "display(titanic.head())\n",
    "print(f\"Columns: {titanic.columns.to_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 'Survived'\n",
    "exclude_vars = ['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin',\n",
    "                'Sex', 'Embarked']\n",
    "\n",
    "X = titanic[[col for col in titanic.columns if col not in exclude_vars]].copy()\n",
    "y = titanic[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f31386",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = np.nanmean(X['Age'])\n",
    "X['Age'] = X['Age'].apply(lambda x: x if not pd.isna(x) else mean_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36ef28",
   "metadata": {},
   "source": [
    "Before we get started with modeling, we will do one more data pre-processing step: **splitting data into training/test/validation sets**.  These sets are defined as follows:\n",
    "  - **Training**: The sample of data used to fit the model.\n",
    "  - **Validation**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "  - **Test**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "There is a handy function to do this in `scikit-learn` that we will use; however, it only splits the data into 2 sets, so we just apply this twice to get the sets we want.  In this case, we will have a training set of 60% of the data, validation set of 20% of the data, and test set of 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=8675309)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.25, # 0.25 x 0.8 = 0.2\n",
    "                                                  random_state=8675309) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74e24d",
   "metadata": {},
   "source": [
    "### Using `statsmodels` to build a logistic regression\n",
    "\n",
    "First, we take a statistical approach to building a model. üßÆÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8162e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b538a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm_constant = sm.add_constant(np.asarray(X_train) )\n",
    "## ^ Note Statsmodels does not include an intercept by default **different from what we will see in sklearn\n",
    "logit = sm.Logit(np.asarray(y_train), X_sm_constant) \n",
    "result = logit.fit() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39e91c",
   "metadata": {},
   "source": [
    "As mentioned, `statsmodels` is best for explaining the relationship between X and Y.  In logistic regression, you can best interpret coefficients using an *odds ratio*.  \n",
    "\n",
    "As we can see:\n",
    "\n",
    "The intercept= -1.12546 which corresponds to the log odds of the probability of being in an honor class p .\n",
    "\n",
    "We can go from the log odds to the odds by exponentiating the coefficient which gives us the odds O=0.3245.\n",
    "\n",
    "We can go backwards to the probability by calculating p=O1+O = 0.245.\n",
    "\n",
    "\n",
    "We can see that:\n",
    "\n",
    "The intercept= -1.47085 which corresponds to the log odds for males being in an honor class (since male is the reference group, female=0).\n",
    "\n",
    "The coefficient for female= 0.59278 which corresponds to the log of odds ratio between the female group and male group. The odds ratio equals 1.81 which means the odds for females are about 81% higher than the odds for males.\n",
    "\n",
    "\n",
    "Odds Ratio: 1.131\n",
    "\n",
    "Interpretation: As X increases by 1, the odds that the passenger survives are 1.13 times greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db130e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = result.predict(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3f44d",
   "metadata": {},
   "source": [
    "### Using `scikit-learn` to build a logistic regression\n",
    "\n",
    "Now we'll take a Machine Learning approach to build a model and we'll compare results ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea8312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e755b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed34caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440cda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3186e89",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As always in Python, there's \"more than one way to skin a cat\".  Depending on your problem, you may opt for either `statsmodels` or `scikit-learn` to build your model.  If you are more interested in generating the best **prediction**, you should probably opt for `scikit-learn`.  On the other hand, if you're more interested in explaining the relationship between X and Y, you should opt for `statsmodels`.  Also, there are key differences in these modules in terms of default parameters (e.g. intercepts and regularization) and other API features that you should be aware of.\n",
    "\n",
    "#### Additional Learning Resources\n",
    "\n",
    " - scikit-learn documentation\n",
    " - statsmodels documentation\n",
    " - Statistical learning book\n",
    " - blog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
